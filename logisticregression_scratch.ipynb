{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.training_accuracies = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def initialize(self,x):\n",
    "        self.weights = np.zeros(x.shape[1])    # number of features , shape of X->mxd and weights -> d\n",
    "        \n",
    "        self.bias = 0\n",
    "       \n",
    "    \n",
    "    def logits(self,x):\n",
    "        return (np.dot(self.weights,x.transpose())+self.bias) \n",
    "        #return (np.matmul(x,self.weights)+self.bias)\n",
    "    \n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return np.array([self.sigmoidFunction(x) for x in z])   \n",
    "    def sigmoidFunction(self,z):\n",
    "        \n",
    "        if z >= 0:\n",
    "            exp_z = np.exp(-z)\n",
    "            return 1/(1+exp_z)\n",
    "        else:\n",
    "            exp_z = np.exp(z)\n",
    "            return (exp_z/(1+exp_z))\n",
    "    #Cost Function/Loss Function\n",
    "    def computeLoss(self,y,yhat):\n",
    "        #binary cross entropy loss \n",
    "        y_zero_loss = y *np.log(yhat+1e-9)\n",
    "        y_one_loss = (1-y)*np.log(1-yhat +1e-9)\n",
    "        return -np.mean(y_zero_loss + y_one_loss)\n",
    "    \n",
    "    def computeGradient(self,x,y_true,y_pred):\n",
    "        difference = y_pred - y_true\n",
    "        gradient_b = np.mean(difference)\n",
    "        gradient_w = np.matmul(x.transpose(),difference)\n",
    "        gradient_w = np.array([np.mean(grad) for grad in gradient_w])\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def updateParameters(self,grad_w, grad_b,lr):\n",
    "        self.weights = self.weights - lr * grad_w\n",
    "        self.bias = self.bias - lr * grad_b\n",
    "        \n",
    "\n",
    "    def fit(self,x,y,epochs = 500):\n",
    "        self.initialize(x)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            #dot product of w,x\n",
    "            z = np.matmul(self.weights, x.transpose()) + self.bias\n",
    "            \n",
    "            #activation function apply\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = self.computeLoss(y,y_pred)\n",
    "\n",
    "            #gradients w.r.t weights and biases\n",
    "            erro_w, error_b = self.computeGradient(x,y,y_pred)\n",
    "\n",
    "            #update weights and biases\n",
    "            self.updateParameters(erro_w,error_b,0.1)\n",
    "\n",
    "            pred_to_class = [1 if p>0.5 else 0 for p in y_pred]\n",
    "            self.training_accuracies.append(accuracy_score(y,pred_to_class))\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            if epoch%100 ==0:\n",
    "                print(f'epoch{epoch} and loss:{loss}')\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1beca4d13f83ca89c66bd45495ae156699bc5ff6f8c7e501c023379b2af8736"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
